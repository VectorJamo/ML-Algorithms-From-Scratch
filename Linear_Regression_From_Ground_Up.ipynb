{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7fT1PzV6ZJXBdB7S/tSB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VectorJamo/ML-Algorithms-From-Scratch/blob/main/Linear_Regression_From_Ground_Up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ChsBxHA8YsD7"
      },
      "outputs": [],
      "source": [
        "# Implementing linear regression from scratch.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A linear regression model finds the line of best fit in n-dimensional space\n",
        "# The equation of this line in 2D is y = wx + b\n",
        "# In n-dimensional space, the equation is, y = w1(x1) + w2(x2) + w3(x3) + .... + w(n-1)x(n-1) + b\n",
        "\n",
        "def get_prediction(feature_vector, weight_vector, bias):\n",
        "  prediction = np.dot(feature_vector, weight_vector) + bias\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "p08XXFqLatDp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It is our goal now to find the appropriate values of the weights and bias such that the cost of each training is minimized.\n",
        "# The cost function is the sum of the mean squared error(MSE) over the training data.\n",
        "# C = sum over the entire dataset(y(i) - ((w)x(i) + b))^2\n",
        "# Here, i = iterator going from 1 to N where N is the no. of data examples in the dataset. Each data example is a pair of (feature vector, output label)\n",
        "# x(i) is the feature vector, w is the weight vector, b is the bias and y(i) is the actual label for that training data.\n",
        "# Hence, this equation can be reduced as the square of the difference of the actual label and the predicted label by the model.\n",
        "# We wish to decrease the this value of the cost function overtime during training. Lesser the difference, we can see that better is our model is predicting\n",
        "# values that are closer to the actual values which is what we want.\n",
        "\n",
        "def get_total_training_cost(feature_vectors, output_labels, weight_vector, bias):\n",
        "  training_size = len(feature_vectors)\n",
        "  training_cost = 0\n",
        "  for i in range(0, training_size):\n",
        "    training_cost += (output_labels[i] - (np.dot(feature_vectors[i], weight_vector) + bias))**2\n",
        "\n",
        "  return training_cost"
      ],
      "metadata": {
        "id": "ZXg1FheBx0Pq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement Gradient Descent\n",
        "# Gradient Descent is a way of nudging the weights and biases in such a way that the cost of the the training decreases.\n",
        "# It consists of finding the gradient vector that points to the direction in which the function is decreasing and then taking steps in that direction.\n",
        "# When this process is done for many many times, we will eventually reach a local minimum where the cost function is minimized.\n",
        "\n",
        "# The equation for getting the derivative of the cost function with respect to the weight dC/dw is just the partial derivative of the cost function with\n",
        "# respect to the weight vector. Computing the partial derivative is trivial and can be done with a pen and paper.\n",
        "def get_weight_gradient(feature_vectors, output_labels, weight_vector, bias): # Returns dC/dw\n",
        "  weight_gradient = np.zeros(len(weight_vector))\n",
        "  training_size = len(feature_vectors)\n",
        "  for i in range(0, training_size):\n",
        "    weight_gradient += (-2 * (output_labels[i] - (np.dot(feature_vectors[i], weight_vector) + bias))) * feature_vectors[i]\n",
        "\n",
        "  return weight_gradient\n",
        "\n",
        "def get_bias_gradient(feature_vectors, output_labels, weight_vector, bias): # Returns dC/db\n",
        "  bias_gradient = 0\n",
        "  training_size = len(feature_vectors)\n",
        "  for i in range(0, training_size):\n",
        "    bias_gradient += (-2 * (output_labels[i] - (np.dot(feature_vectors[i], weight_vector) + bias)))\n",
        "\n",
        "  return bias_gradient\n",
        "\n",
        "def perform_gradient_descent(feature_vectors, output_labels, weight_vector, bias):\n",
        "  dw = get_weight_gradient(feature_vectors, output_labels, weight_vector, bias)\n",
        "  db = get_bias_gradient(feature_vectors, output_labels, weight_vector, bias)\n",
        "  learning_rate = 0.01\n",
        "\n",
        "  weight_vector -= learning_rate * dw\n",
        "  bias -= learning_rate * db"
      ],
      "metadata": {
        "id": "5ybEQ6hk26Cg"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LRParameters():\n",
        "  def __init__(self):\n",
        "    self.weights = 0\n",
        "    self.bias = 0\n",
        "\n",
        "def train(feature_vectors, output_labels, epochs):\n",
        "  model_parameters = LRParameters()\n",
        "\n",
        "  # Initial model parameters\n",
        "  np.random.seed(40)\n",
        "  model_parameters.weights = np.random.rand(len(feature_vectors[0]))\n",
        "  model_parameters.bias = np.random.rand(1)\n",
        "  print(f'Initial weights: {model_parameters.weights}')\n",
        "  print(f'Initial bias: {model_parameters.bias}')\n",
        "\n",
        "  for i in range(0, epochs):\n",
        "    # Calculate the training cost\n",
        "    training_cost = get_total_training_cost(feature_vectors, output_labels, model_parameters.weights, model_parameters.bias)\n",
        "\n",
        "    # Perform gradient descent\n",
        "    perform_gradient_descent(feature_vectors, output_labels, model_parameters.weights, model_parameters.bias)\n",
        "\n",
        "    print(f'Training round: {i}. Training cost: {training_cost}')\n",
        "    print(f'Weights: {model_parameters.weights}. Bias: {model_parameters.bias}')\n",
        "    print('-------------------------------------------------------------------')\n",
        "\n",
        "  return model_parameters\n"
      ],
      "metadata": {
        "id": "SiRE_nMJ8R_0"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "feature_vectors = [[1, 2, 3]]\n",
        "output_labels = [[5]]\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "SfZ-T51k5dOx"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(feature_vectors, output_labels, epochs)\n",
        "print(f'New weights: {model.weights}')\n",
        "print(f'New bias: {model.bias}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tjpCOu795uQ",
        "outputId": "cbf8e462-f165-410e-8f25-bb0c0d229f09"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights: [0.40768703 0.05536604 0.78853488]\n",
            "Initial bias: [0.28730518]\n",
            "Training round: 0. Training cost: [3.3440379]\n",
            "Weights: [0.44426045 0.12851288 0.89825514]. Bias: [0.32387861]\n",
            "-------------------------------------------------------------------\n",
            "Training round: 1. Training cost: [1.63857857]\n",
            "Weights: [0.46986184 0.17971567 0.97505933]. Bias: [0.34948]\n",
            "-------------------------------------------------------------------\n",
            "Training round: 2. Training cost: [0.8029035]\n",
            "Weights: [0.48778282 0.21555763 1.02882226]. Bias: [0.36740098]\n",
            "-------------------------------------------------------------------\n",
            "Training round: 3. Training cost: [0.39342271]\n",
            "Weights: [0.5003275  0.24064699 1.06645631]. Bias: [0.37994566]\n",
            "-------------------------------------------------------------------\n",
            "Training round: 4. Training cost: [0.19277713]\n",
            "Weights: [0.50910878 0.25820955 1.09280014]. Bias: [0.38872694]\n",
            "-------------------------------------------------------------------\n",
            "Training round: 5. Training cost: [0.09446079]\n",
            "Weights: [0.51525568 0.27050334 1.11124083]. Bias: [0.39487383]\n",
            "-------------------------------------------------------------------\n",
            "Training round: 6. Training cost: [0.04628579]\n",
            "Weights: [0.5195585  0.27910899 1.12414931]. Bias: [0.39917666]\n",
            "-------------------------------------------------------------------\n",
            "Training round: 7. Training cost: [0.02268004]\n",
            "Weights: [0.52257048 0.28513295 1.13318524]. Bias: [0.40218864]\n",
            "-------------------------------------------------------------------\n",
            "Training round: 8. Training cost: [0.01111322]\n",
            "Weights: [0.52467887 0.28934972 1.1395104 ]. Bias: [0.40429702]\n",
            "-------------------------------------------------------------------\n",
            "Training round: 9. Training cost: [0.00544548]\n",
            "Weights: [0.52615474 0.29230146 1.14393801]. Bias: [0.40577289]\n",
            "-------------------------------------------------------------------\n",
            "New weights: [0.52615474 0.29230146 1.14393801]\n",
            "New bias: [0.40577289]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize\n"
      ],
      "metadata": {
        "id": "vFORbYuO--BS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}